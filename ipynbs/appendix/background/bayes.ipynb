{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯公式\n",
    "\n",
    "贝叶斯是机器学习的核心方法之一.这背后的深刻原因在于,现实世界本身就是不确定的,人类的观察能力是有局限性的,我们日常所观察到的只是事物表面上的结果,这个时候，我们就需要提供一个假说(hypothesis).所谓假说当然就是不确定是不是对的,往往假说只是能满足当前看到的表象而已,但也绝对不是两眼一抹黑瞎蒙.贝叶斯公式提供了这样一种科学构建假说的方法.\n",
    "\n",
    "具体地说，我们需要做两件事情:\n",
    "\n",
    "1. 算出各种不同猜测的可能性大小.\n",
    "\n",
    "    专业点说,对于离散时间计算特定事件的后验概率.对于连续的猜测空间则是计算猜测的概率密度函数\n",
    "    \n",
    "    \n",
    "2. 算出最靠谱的猜测是什么.\n",
    "\n",
    "    专业点说,就是所谓的模型比较，模型比较如果不考虑先验概率的话就是最大似然方法.\n",
    "\n",
    "祖师爷托马斯·贝叶斯(Thomas Bayes)([维基](https://en.wikipedia.org/wiki/Thomas_Bayes)上有他的详细经历)并没有能在生前发表它的贝叶斯公式的.而是在死后由它的一个朋友发表了这个伟大的成果,相当遗憾.\n",
    "\n",
    "\n",
    "## 贝叶斯公式的数学表示\n",
    "\n",
    "以离散情况为例\n",
    "\n",
    "$$ P(Y|X)=\\frac{P(X|Y)*P(Y)}{P(X)} $$\n",
    "\n",
    "\n",
    "现实中便是用\n",
    "\n",
    "$\\forall Y_k \\in Y, \\vec{x} = (X_{1,j_1},X_{2,j_2},...,X_{d,j_d} \\in X)$,其中$X_{i,j}$就是第i个预测变量的第j种情况.\n",
    "\n",
    "的频数逼近条件概率$P(X|Y) $来计算出现了$\\vec{x}$特征属于$Y_k$类的概率.\n",
    "\n",
    "## 条件概率\n",
    "\n",
    "我们知道${P(X)}$表示X发生的概率,这种被称为无条件概率;而$P(X|Y)$则被称为条件概率,所谓条件概率是指一个事件发生的条件下另一个事件发生的概率;而两个事件同时发生的概率叫做联合概率($P(XY)$).这里面几个概率容易混淆,下面用一个例子来讲解,比如:\n",
    "\n",
    "我们假设生个男娃(事件对应无条件概率$P(X)$)的概率是50%,人的血型为B型(事件对应无条件概率$P(A)$)的概率是35%,而生了个男娃的情况下这个娃血型是B型的概率(事件对应条件概率$P(A|X)$)是40%.那生个B型血男娃(事件对应联合概率$P(AX)$)的概率就是20%.\n",
    "\n",
    "条件概率的计算公式是:\n",
    "\n",
    "$$ P(A|X) = \\frac {P(AX)}{P(X)}$$\n",
    "\n",
    "\n",
    "条件概率链式法则是条件概率定义的扩展:\n",
    "\n",
    "$$ P(AB)=P(B)P(A|B)=P(A)P(B|A) $$\n",
    "\n",
    "也就是说某两件事同时发生的概率是其中一件事发生的概率乘以这件事发生后也发生另一件事的概率.\n",
    "\n",
    "如果有三个事件那么:\n",
    "\n",
    "$$ P(ABC)=P(AB)P(C|AB)=P(A)P(B|A)P(C|AB) $$\n",
    "\n",
    "因此推广到N件事就有\n",
    "\n",
    "$$ P(A_1A_2...A_n)=P(A_1)P(A_2|A_1)P(A_3|A_1A_2)...P(A_n|A_1A_2...A_{n-1})$$\n",
    "\n",
    "\n",
    "## 贝叶斯公式解决什么问题\n",
    "\n",
    "这么说有可能太过数学,那么换一种方式我们来看看贝叶斯公式解决的是什么问题.\n",
    "\n",
    "\n",
    "***例1:假设我们的袋子里有10个白球15个黑球,白球全部没有数字,但黑球有5个有数字8(9球中黑球编号为8)那么摸出一个不带数字的黑球的概率有多少?***\n",
    "\n",
    "\n",
    "*这个问题很基础,结果是$\\frac{15-5} {15+10} = \\frac 2 5$*\n",
    "\n",
    "\n",
    "这是个正向的过程,已知分布求概率,那么很自然的,如果反过来呢\n",
    "\n",
    "***例2:如果我们知道黑球15个白球10个，白球全部没有数字,但黑球有5个有数字8(9球中黑球编号为8),但我们是'黑白色盲'知道上面没有数字,那么摸到的是黑球的概率是多少?***\n",
    "\n",
    "这就是贝叶斯公式尝试解决的问题.利用贝叶斯公式.我们带进去算,摸到没字的球的情况下是黑球的概率\n",
    "\n",
    "$ {P(黑)} = \\frac {15}{15+10}=\\frac 3 5 $\n",
    "\n",
    "$ {P(没字)} = \\frac {10+15-5}{15+10}=\\frac 4 5 $\n",
    "\n",
    "$ {P(没字|黑)} = \\frac {15-5}{15}=\\frac 2 3 $\n",
    "\n",
    "$ {P(黑|没字)} = \\frac {P(没字|黑)*P(黑)}{P(没字)}=\\frac {\\frac 2 3* \\frac 3 5} {\\frac 4 5}=\\frac 1 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 贝叶斯学派\n",
    "\n",
    "说到贝叶斯公式就肯定会联想到贝叶斯学派.\n",
    "\n",
    "关于对概率的定义,总的来说有两种不同的认识学派,一种叫频率派,一种叫贝叶斯派.他们在根源上对待概率,随机性的定义就有分歧.\n",
    "\n",
    "+ 频率派从「自然」角度出发,试图直接为「事件」本身建模,即事件A在独立重复试验中发生的频率趋于极限p,那么这个极限就是该事件的概率.举例而言,想要计算抛掷一枚硬币时正面朝上的概率,我们需要不断地抛掷硬币,当抛掷次数趋向无穷时正面朝上的频率即为正面朝上的概率.这种思维方式很朴素很自然.\n",
    "\n",
    "+ 贝叶斯学派并不从试图刻画「事件」本身,而从「观察者」角度出发.贝叶斯学派并不试图说「事件本身是随机的」,或者「世界的本体带有某种随机性」,这套理论根本不言说关于「世界本体」的东西,而只是从「观察者知识不完备」这一出发点开始,构造一套在贝叶斯概率论的框架下可以对不确定知识做出推断的方法.频率学派下说的「随机事件」在贝叶斯学派看来,并不是「事件本身具有某种客观的随机性」,而是「观察者不知道事件的结果」而已,只是「观察者」知识状态中尚未包含这一事件的结果.但是在这种情况下,观察者又试图通过已经观察到的「证据」来推断这一事件的结果,因此只能靠猜.贝叶斯概率论就想构建一套比较完备的框架用来描述最能服务于理性推断这一目的的「猜的过程」.因此，在贝叶斯框架下,同一件事情对于知情者而言就是「确定事件」,对于不知情者而言就是「随机事件」,随机性并不源于事件本身是否发生,而只是描述观察者对该事件的知识状态.\n",
    "\n",
    "\n",
    "可以看出.频率学派将概率看作一种客观存在,而贝叶斯学派则将概率理解为是一种观察行为的认识结果.究竟谁对谁错我们暂且不讨论.总之这两派目前都能自圆其说我们就当作是对一个事务的两种合理解释好了.\n",
    "\n",
    "当然由于这一分歧,频率派和贝叶斯派研究问题的方法也就不同了.\n",
    "\n",
    "+ 频率派把需要推断的概率$\\theta$是固定的未知常数,即概率虽然是未知的,但最起码是确定的一个值,同时,样本X是随机的,所以频率派重点研究样本空间,大部分的概率计算都是针对样本X的分布；\n",
    "\n",
    "+ 而贝叶斯派的观点则截然相反,他们认为要求的概率$\\theta$是随机的,而样本X是固定的,由于样本是固定的,所以他们重点研究的是这个要求的随机概率$\\theta$的分布.\n",
    "\n",
    "\n",
    "贝叶斯学派为什么会叫贝叶斯学派(上文说过贝叶斯在火之前就早死了,不可能开宗立派).这主要是和贝叶斯公式隐含的思维方式有关.我们说过贝叶斯公式的基本思维方式就是根据已有的观察现象推测可能的概率,它依赖事先定好的非条件概率和由样本信息决定的某事件发生概率,而得到的是一个推测的概率.以公式中的符号作为描述的符号:\n",
    "\n",
    "$$ P(Y|X)=\\frac{P(X|Y)*P(Y)}{P(X)} $$\n",
    "\n",
    "其中无条件概率有$P(X)$和$P(Y)$,由样本信息决定的概率有$P(X|Y)$,要求的概率有$P(Y|X)$.\n",
    "\n",
    "+ $P(X)$和$P(Y)$被称作先验概率(prior probability),意思是这个概率并不是由某个样本集中来,而是一早被认定的概率并不是从样本中提取的信息.当然,针对于例子2中的情况,先验概率也是从样本中提取的,这里使用的是边缘概率(Marginal Probability)来作为先验概率.边缘概率是某个事件独立发生的概率.边缘概率是这样得到的:在联合概率中,把最终结果中不需要的那些事件合并成其事件的全概率而消去(对离散随机变量用求和得全概率,对连续随机变量用积分得全概率)这一过程也被称作边缘化(marginalization).\n",
    "\n",
    "+ 而要求的$P(Y|X)$和从样本信息中提取的概率$P(X|Y)$叫做后验概率(posterior probability),意思是这个概率是由样本信息中推测出来的.其中$P(Y|X)$称作Y的后验概率,$P(X|Y)$称作X的后验概率.\n",
    "\n",
    "那贝叶斯公式就可以看作是一个通过先验概率和样本信息推测后验概率的公式.再一推广,只要知道了先验的概率分布和样本的信息,那我们就可以以此推断出一个后验的概率分布.这一思考模式就可以指导我们对许多问题做出合理推断了.我想这就是为什么会叫贝叶斯学派吧.\n",
    "\n",
    "\n",
    "贝叶斯派提出的一个思考问题的固定模式：\n",
    "\n",
    "$$ 先验分布(prior distribution) + 样本信息 = 后验分布(posterior distributions) $$\n",
    "\n",
    "\n",
    "可能这一思考模式相比起贝叶斯公式更加有意义.由这一思考模式我们可以看出,在已知先验分布的时候,样本信息就可以决定后验分布,换言之,在得到新的样本信息之前,人们对的认知是先验分布,在得到新的样本信息后,人们会刷新认知,因此样本信息越多我们的后验概率的推测就会越接近真实.\n",
    "\n",
    "而后验分布一般也认为是在给定样本的情况下的条件分布,而使达到最大的值称为最大后验估计,类似于经典统计学中的极大似然估计.\n",
    "\n",
    "综合起来看,则好比是人类刚开始时对大自然只有少得可怜的先验知识,但随着不断是观察,实验获得更多的样本,结果,使得人们对自然界的规律摸得越来越透彻.所以,贝叶斯方法既符合人们日常生活的思考方式,也符合人们认识自然的规律.\n",
    "\n",
    "\n",
    "## 写在最后\n",
    "\n",
    "这篇文章只是抛砖引玉,想要了解更多贝叶斯方法的知识和贝叶斯学派的知识可以看看经典著作'PRML',国内哈工大有一位大神马春鹏对其进行了翻译,电子版网上很好找,水准很高推荐一读,同时感慨下学习计算机,机器学习相关专业真是一件幸福的事情.这个学科开源开放的精神其他学科完全没法比."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
