{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "\n",
    "Dropout是一种用于深度神经网络的方法,用于避免过拟合.  \n",
    "在训练时,每次迭代中每层以提前设定的概率,称为keep_prob,来随机选择保留的节点,其他的节点在该次前向传播被忽略(设为0),同时在后向传播中也忽略.如下图中打叉的节点就是某次迭代中忽略的节点.  \n",
    "\n",
    "\n",
    "![dropout示例](dropout.jpeg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作用,应用  \n",
    "\n",
    "Dropout主要有两个作用:  \n",
    "\n",
    "1. dropout在训练时每个迭代中相当于减小了网络规模,有正则化的作用.  \n",
    "2. dropout可以避免把过多的权重放在某个节点上,而是把权重分散给全部的节点.   \n",
    "\n",
    "在实践中,dropout在CV领域比较有效,因为数据输入一般是图片像素,维度很高,而网络层的形状,一般和输入形状相关,往往也很大."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverted Dropout   \n",
    "\n",
    "传统的dropout方法在训练时随机扔掉的节点,利用反向传播来更新保留的节点.但是在验证和测试的时候,会为每个节点乘以所在层的保留概率(keep_prob),这个操作叫做scale. 试想,每次使用模型预测的时候我们都要scale,比较麻烦.\n",
    "\n",
    "Hinton等人提出inverted dropout,也就是在训练过程中,把保留的节点除以keep_prob,叫做inverted scale,来弥补随机扔掉的节点.这样,在验证和测试的时候,就不需要scale了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考  \n",
    "\n",
    "- [吴恩达老师解释dropout动机的视频](https://www.youtube.com/watch?v=ARq74QuavAo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "do_dropout = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用pytorch来实现,尽量做到每层保留的节点为keep_prob * 节点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1620, -0.3637, -1.0468, -0.5357],\n",
       "        [-0.9185,  1.8240,  1.3015,  0.1691],\n",
       "        [ 0.8552, -0.1683,  1.2044, -0.3275],\n",
       "        [ 1.7480,  0.3354, -0.8424, -0.1835]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "w1 = torch.randn(4, 4)  # 某层的weights\n",
    "w = copy.deepcopy(w1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_strict(w, keep_prob):\n",
    "    \"\"\"implement inverted dropout ensuring that the share of kept neurons is strictly keep_prob.\n",
    "    \n",
    "    Args:\n",
    "        w (torch.tensor) : weights before dropout\n",
    "        keep_prob(float) : keep probability\n",
    "    \"\"\"\n",
    "    k = round(w.shape[1] * keep_prob)\n",
    "    _, indices = torch.topk(torch.randn(w.shape[0], w.shape[1]), k)  \n",
    "    keep = torch.zeros(4, 4).scatter_(dim=1, index=indices, src=torch.ones_like(w))  \n",
    "    w *= keep\n",
    "    w /= keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.0000, -2.0936, -1.0714],\n",
      "        [-0.0000,  3.6479,  2.6031,  0.0000],\n",
      "        [ 1.7104, -0.0000,  2.4087, -0.0000],\n",
      "        [ 3.4961,  0.0000, -1.6848, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "if do_dropout:\n",
    "    dropout_strict(w, keep_prob)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用numpy来实现,比较简单,当节点数量大时,随机的结果基本能够保证实际保留情况符合保留概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10960301,  0.03820597, -1.5917774 , -0.65663746],\n",
       "       [-0.10249081,  0.90052467,  1.39159973,  1.35458563],\n",
       "       [-1.57200004, -0.04055162, -1.01190991,  0.54858   ],\n",
       "       [ 0.93414654,  0.52328988,  1.34527992, -0.11895916]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy \n",
    "w1 = np.random.randn(4, 4)  # 某层的weights\n",
    "w = copy.deepcopy(w1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_loose(w, keep_prob):\n",
    "    \"\"\"A simple Implementation of inverted dropout.\n",
    "    \n",
    "    Args:\n",
    "        w(np.array) :- neurons subject to dropout\n",
    "        keep_prob(float) :- keep probability\n",
    "    \"\"\"\n",
    "    keep = np.random.rand(w.shape[0], w.shape[1]) < keep_prob\n",
    "    w *= keep\n",
    "    w /= keep_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10960301  0.03820597 -1.5917774  -0.65663746]\n",
      " [-0.10249081  0.90052467  1.39159973  1.35458563]\n",
      " [-1.57200004 -0.04055162 -1.01190991  0.54858   ]\n",
      " [ 0.93414654  0.52328988  1.34527992 -0.11895916]]\n",
      "[[0.13311383 0.65956675 0.46623683 0.65456941]\n",
      " [0.09542607 0.2339979  0.436658   0.67150385]\n",
      " [0.39182677 0.73311865 0.31554844 0.94106821]\n",
      " [0.86621335 0.78854791 0.67639935 0.81278023]]\n",
      "0.5\n",
      "[[ True False  True False]\n",
      " [ True  True  True False]\n",
      " [ True False  True False]\n",
      " [False False False False]]\n",
      "[[-0.10960301  0.         -1.5917774  -0.        ]\n",
      " [-0.10249081  0.90052467  1.39159973  0.        ]\n",
      " [-1.57200004 -0.         -1.01190991  0.        ]\n",
      " [ 0.          0.          0.         -0.        ]]\n",
      "[[-0.21920601  0.         -3.1835548  -0.        ]\n",
      " [-0.20498163  1.80104933  2.78319947  0.        ]\n",
      " [-3.14400009 -0.         -2.02381982  0.        ]\n",
      " [ 0.          0.          0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "if do_dropout:\n",
    "    dropout_loose(w, keep_prob)\n",
    "    print(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
