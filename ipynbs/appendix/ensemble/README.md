# 集成方法

集成方法的核心思想可以用一句话说明:`三个臭皮匠顶个诸葛亮`.集成方法的目标是把多个使用给定学习算法构建的基估计器的预测结果结合起来，从而获得比单个估计器更好的泛化能力,鲁棒性.

集成方法通常分为两种:

+ 平均方法，该方法的原理是构建多个独立的估计器，然后取它们的预测结果的平均。一般来说组合之后的估计器是会比单个估计器要好的，因为它的方差减小了。

例如:Bagging方法, 随机森林等

+ 相比之下，在 boosting 方法中，基估计器是依次构建的，并且每一个基估计器都尝试去减少组合估计器的偏差.这种方法主要目的是为了结合多个弱模型,使集成的模型更加强大.

例如: AdaBoost,梯度提升树等

当然另一中算不上算法但也很有用的集成方式就是投票(voting)了.投票的话就是直接把多个模型的实例集合在一起进行投票,票多的就作为结果,当然也可以为各个模型实例加权.