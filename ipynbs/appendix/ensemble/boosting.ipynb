{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升算法\n",
    "\n",
    "提升算法的核心不再是投票,而是迭代.通过一轮一轮的优化权重来做到提升准确率的作用\n",
    "\n",
    "提升算法(Boosting)虽然表面上看也是对训练数据的扰动(重采样/权值调整),但是Boosting的理论保证了其本质上是一个优化算法.集成分类器整体具有一个优化目标,即Boosting的训练过程最终可以使集成分类器收敛到最优贝叶斯决策,因此降低了bias(提高了准确度),而这个性质是Bagging不具有的.\n",
    "\n",
    "\n",
    "常见的提升算法就是\n",
    "\n",
    "+ `AdaBoost`\n",
    "+ `GradientBoosting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost的核心思想是用反复修改的数据的权重来训练一系列的弱学习器,由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合,得到我们最终的预测结果.在每一次所谓的提升迭代中,数据的修改由应用于每一个训练样本的新权重$w_1$,$w_2$,…,$w_N$组成(即修改每一个训练样本应用于新一轮学习器的权重). 初始化时,将所有弱学习器的权重都设置为$w_i = 1/N$,因此第一次迭代仅仅是通过原始数据训练出一个弱学习器.在接下来的连续迭代中,样本的权重逐个地被修改,学习算法也因此要重新应用这些已经修改的权重.在给定的一个迭代中,那些在上一轮迭代中被预测为错误结果的样本的权重将会被增加,而那些被预测为正确结果的样本的权重将会被降低.随着迭代次数的增加,那些难以预测的样例的影响将会越来越大,每一个随后的弱学习器都将会被强迫更加关注那些在之前被错误预测的样例."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***AdaBoost在sklearn中的接口*** \n",
    "\n",
    "\n",
    "\n",
    "参数方面\n",
    "弱学习器的数量由参数`n_estimators`来控制.`learning_rate` 参数用来控制每个弱学习器对 最终的结果的贡献程度（校对者注：其实应该就是控制每个弱学习器的权重修改速率，这里不太记得了，不确定）。 弱学习器默认使用决策树。不同的弱学习器可以通过参数 base_estimator 来指定。 获取一个好的预测结果主要需要调整的参数是 n_estimators 和 base_estimator 的复杂度 (例如:对于弱学习器为决策树的情况，树的深度 max_depth 或叶子节点的最小样本数 min_samples_leaf 等都是控制树的复杂度的参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
