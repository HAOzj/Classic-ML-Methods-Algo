{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升算法\n",
    "\n",
    "提升算法的核心不再是投票,而是迭代.通过一轮一轮的优化权重来做到提升准确率的作用\n",
    "\n",
    "提升算法(Boosting)虽然表面上看也是对训练数据的扰动(重采样/权值调整),但是Boosting的理论保证了其本质上是一个优化算法.集成分类器整体具有一个优化目标,即Boosting的训练过程最终可以使集成分类器收敛到最优贝叶斯决策,因此降低了bias(提高了准确度),而这个性质是Bagging不具有的.\n",
    "\n",
    "\n",
    "常见的提升算法就是\n",
    "\n",
    "+ `AdaBoost`\n",
    "+ `GradientBoosting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost的核心思想是用反复修改的数据的权重来训练一系列的弱学习器,由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合,得到我们最终的预测结果.在每一次所谓的提升迭代中,数据的修改由应用于每一个训练样本的新权重$w_1$,$w_2$,…,$w_N$组成(即修改每一个训练样本应用于新一轮学习器的权重). 初始化时,将所有弱学习器的权重都设置为$w_i = 1/N$,因此第一次迭代仅仅是通过原始数据训练出一个弱学习器.在接下来的连续迭代中,样本的权重逐个地被修改,学习算法也因此要重新应用这些已经修改的权重.在给定的一个迭代中,那些在上一轮迭代中被预测为错误结果的样本的权重将会被增加,而那些被预测为正确结果的样本的权重将会被降低.随着迭代次数的增加,那些难以预测的样例的影响将会越来越大,每一个随后的弱学习器都将会被强迫更加关注那些在之前被错误预测的样例."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***AdaBoost在sklearn中的接口*** \n",
    "\n",
    "sklearn中提供了俩个adaboost相关的接口:\n",
    "\n",
    "+ `ensemble.AdaBoostClassifier([…])`adaboost 分类器\n",
    "+ `ensemble.AdaBoostRegressor([base_estimator, …])`adaboost 回归器\n",
    "\n",
    "参数方面\n",
    "\n",
    "+ `base_estimator`指定弱学习器,默认是cart tree,\n",
    "\n",
    "+ `n_estimators`弱学习器的数量\n",
    "\n",
    "+ `learning_rate`控制每个弱学习器对最终的结果的贡献程度(权重修改速率)\n",
    "\n",
    "获取一个好的预测结果主要需要调整的参数是`n_estimators`和`base_estimator`的复杂度(例如:对于弱学习器为决策树的情况,树的深度`max_depth` 或叶子节点的最小样本数`min_samples_leaf`等都是控制树的复杂度的参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_content = requests.get(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\").text\n",
    "row_name = ['sepal_length','sepal_width','petal_length','petal_width','label']\n",
    "csv_list = csv_content.strip().split(\"\\n\")\n",
    "row_matrix = [line.strip().split(\",\") for line in csv_list]\n",
    "dataset = pd.DataFrame(row_matrix,columns=row_name)\n",
    "\n",
    "encs = {}\n",
    "encs[\"feature\"] = StandardScaler()\n",
    "encs[\"feature\"].fit(dataset[row_name[:-1]])\n",
    "table = pd.DataFrame(encs[\"feature\"].transform(dataset[row_name[:-1]]),columns=row_name[:-1])\n",
    "\n",
    "encs[\"label\"]=LabelEncoder()\n",
    "encs[\"label\"].fit(dataset[\"label\"])\n",
    "table[\"label\"] = encs[\"label\"].transform(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.032057</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.337848</td>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.263460</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.537178</td>\n",
       "      <td>1.957669</td>\n",
       "      <td>-1.170675</td>\n",
       "      <td>-1.050031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.800654</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.181504</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>0.800654</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.748856</td>\n",
       "      <td>-0.356361</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.444450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  label\n",
       "0     -0.900681     1.032057     -1.341272    -1.312977      0\n",
       "1     -1.143017    -0.124958     -1.341272    -1.312977      0\n",
       "2     -1.385353     0.337848     -1.398138    -1.312977      0\n",
       "3     -1.506521     0.106445     -1.284407    -1.312977      0\n",
       "4     -1.021849     1.263460     -1.341272    -1.312977      0\n",
       "5     -0.537178     1.957669     -1.170675    -1.050031      0\n",
       "6     -1.506521     0.800654     -1.341272    -1.181504      0\n",
       "7     -1.021849     0.800654     -1.284407    -1.312977      0\n",
       "8     -1.748856    -0.356361     -1.341272    -1.312977      0\n",
       "9     -1.143017     0.106445     -1.284407    -1.444450      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集拆分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set,validation_set = train_test_split(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab= AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab.fit(train_set[row_name[:-1]], train_set[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre = ab.predict(validation_set[row_name[:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       0.85      0.92      0.88        12\n",
      "          2       0.86      0.75      0.80         8\n",
      "\n",
      "avg / total       0.92      0.92      0.92        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(validation_set[\"label\"],pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度提升(Gradient Boosting)\n",
    "\n",
    "梯度提升算法和adaboost类似,但有几个区别:\n",
    "\n",
    "+ adaboost对于每个样本有一个权重,样本预估误差越大,权重越大\n",
    "+ gradient boosting则是直接用梯度拟合残差,没有样本权重的概念\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Gradient Boosting在sklearn中的接口*** \n",
    "\n",
    "sklearn中提供了俩个梯度提升相关的接口:\n",
    "\n",
    "+ `ensemble.GradientBoostingClassifier([loss, …])`\t梯度提升分类器\n",
    "+ `ensemble.GradientBoostingRegressor([loss, …])`\t梯度提升回归器\n",
    "\n",
    "其参数用法和adaboost的接口类似,只是多了个loss参数用于设定使用什么作为损失函数,有两个可选的损失函数:\n",
    "\n",
    "+ deviance指使用logistic作为损失函数,也是默认值\n",
    "+ exponential就和adaboost一样了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        18\n",
      "          1       0.85      0.92      0.88        12\n",
      "          2       0.86      0.75      0.80         8\n",
      "\n",
      "avg / total       0.92      0.92      0.92        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gb= GradientBoostingClassifier(n_estimators=100)\n",
    "gb.fit(train_set[row_name[:-1]], train_set[\"label\"])\n",
    "pre = gb.predict(validation_set[row_name[:-1]])\n",
    "print(classification_report(validation_set[\"label\"],pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost\n",
    "\n",
    "xgboost算法比较复杂,针对传统GBDT算法做了很多细节改进,包括损失函数,正则化,切分点查找算法优化,稀疏感知算法,并行化算法设计等等.\n",
    "\n",
    "相关文献资料:\n",
    "\n",
    "+ [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/pdf/1603.02754v1.pdf)\n",
    "+ [XGBoost Parameters (official guide)](http://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters)\n",
    "\n",
    "这个方法不是sklearn中的算法,要使用它需要安装额外的包,如果是linux或者osx,那么可以使用pip直接安装,如果是windows可以去这个[网址下载其动态链接库](http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/)后使用git安装.\n",
    "\n",
    "其接口和sklearn的风格一致,此处就不做更多介绍了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
