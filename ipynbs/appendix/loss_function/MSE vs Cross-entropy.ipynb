{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类模型的损失函数有很多种,比如\n",
    "- svm的hinge loss\n",
    "- 逻辑回归的logit loss\n",
    "- MSE\n",
    "- 多分类时的交叉熵\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 均方损失/MSE (Mean Square Error)\n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum \\limits_{i=1}^{N} (\\hat{y}_i - y_i)^2  $$\n",
    "其中$N$为样本量,$y_i和\\hat{y}_i$分别为第i个样本真值值和预测值.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 交叉熵方法 (Cross-entropy method)\n",
    "交叉熵方法基于信息论中[交叉熵](https://en.wikipedia.org/w/index.php?title=Cross_entropy&oldid=491011306)的概念,反映预测概率分布和真实概率分布的差别.多分类模型下公式为\n",
    "$$  \\frac{1}{N} \\sum \\limits_{i=1}^{N} \\sum \\limits_{k=1}^{K} p_i^k * log q_i^k $$\n",
    "其中K为多分类问题中类别总数,$p_i^k和q_i^k$分别是第i个样例对于k类的真实值和预测值.往往每个样例的概率分布为one-hot型,也就是每个样例都属于某类.所以上式等于\n",
    "$$ \\frac{1}{N} \\sum \\limits_{i=1}^{N} log q_i^{k_i} $$\n",
    "其中$k_i$为第i个样例的类别.  \n",
    "在二分类模型中,不妨说类别是1和0, 也就是$y \\in (1, 0)$.一般来说,模型的预测值是样例为1类的预测概率.这样当$y_i=1时, q_i^{k_i} = log \\hat{y}_i;当y_i=0时,q_i^{k_i} = log (1-\\hat{y}_i)$.不论$y_i$是0还是1, $q_i^{k_i} = y_i*log\\hat{y}_i + (1-y_i) * log (1-\\hat{y_i})$\n",
    "上式就等于\n",
    "$$  \\frac{1}{N} \\sum \\limits_{i=1}^{N} y_i*log\\hat{y}_i + (1-y_i) * log (1-\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
