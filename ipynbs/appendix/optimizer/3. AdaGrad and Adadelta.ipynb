{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad(Adaptive Gradient)\n",
    "\n",
    "AdaGrad是2011年发表的一种亚梯度方法,论文地址为: http://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf. 在很多大规模问题上验证了有效性.\n",
    "它的迭代为:\n",
    "\n",
    " $$sum_{t+1} = sum_t + (g_{t+1})^2 $$\n",
    " $$\\Delta x_t = - \\eta * \\frac{1}{(\\sqrt{sum_{t+1}} + \\epsilon)}* g_{t+1}  $$\n",
    "\n",
    "关于数学符号,请看\"1. Standard GD and Momentum\".\n",
    "\n",
    "\n",
    "AdaGrad有类似于learning rate annealing的效果,不过学习率衰减不是迭代次数的函数,而是梯度平方和的平方根的函数.\n",
    "预期达到的效果就是:\n",
    "- 之前梯度相对比较大的维度上更新幅度相对小.不同的维度的更新幅度的差距会减小\n",
    "- 随着梯度平方和的增加,更新幅度会越来越小,接近0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AdaGrad不同超参数下对比1](AdaGrad1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](AdaGrad2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缺点:\n",
    "- 分母持续增大,更新幅度会趋近于0,后期停滞\n",
    "- 梯度的量级被消除,更新量对学习率很敏感\n",
    "- 算法对学习率 $\\eta$ 很敏感,参考上面不同量级的学习率下算法的表现\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "\n",
    "Adadelta是2012年作者在Google实习时提出的对AdaGrad的改进.论文地址为: https://arxiv.org/abs/1212.5701. \n",
    "\n",
    "每次迭代为\n",
    "$$E[g^2]_{t+1} = \\rho * E[g^2]_t + (1 - \\rho) * (g_{t+1})^2 $$\n",
    "$$RMS(g)_{t+1} = \\sqrt{ E[g^2]_{t+1} + \\epsilon} $$\n",
    "$$E[(\\Delta x)^2]_t = \\rho * E[(\\Delta x)^2]_{t-1} + (1 - \\rho) * (\\Delta x_{t-1})^2 $$\n",
    "$$RMS(\\Delta x)_t = \\sqrt{ E[(\\Delta x)^2]_t + \\epsilon} $$\n",
    "$$\\Delta x_t = - g_{t+1} * \\frac{RMS(\\Delta x)_t}{RMS(g)_{t+1}} $$\n",
    "\n",
    "其中 $\\epsilon$ 不仅为了稳定数值计算,同时也是为了初始化 $RMS(\\Delta x)_t $.\n",
    "Adadelta对于AdaGrad的改进主要是\n",
    "- 对于每个维度,用梯度平方的指数加权平均代替了至今全部梯度的平方和,避免了后期更新时更新幅度逐渐趋近于0的问题\n",
    "- 用更新量的平方的指数加权平均来动态得代替了全局的标量的学习率,避免了对学习率的敏感\n",
    "- 同时,文章作者提出Adadelta保证了更新量的量纲和参数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Adadelta1.jpg)\n",
    "\n",
    "![](Adadelta2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是,实验中发现不需要人工设定的学习率,但是对 $\\epsilon$ 很敏感,因为第一步的步长是 $\\sqrt{\\epsilon} $\n",
    "- 小了的话,前期步长很小\n",
    "- 大了的话,后期容易引起震荡\n",
    "\n",
    "作为对比,以下是带Momentum的GD和RMSProp的效果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GDM_RMSProp.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
