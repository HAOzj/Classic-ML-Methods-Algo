{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "在这里给大家罗列一下监督学习中常见的概念.\n",
    "\n",
    "* 带标签的d维数据:数据有d个预测变量和标签,$\\{ \\vec{x}, y:\\vec{x} \\in R^d, y\\in Y \\}$,其中$Y$为标签集\n",
    "* 二元和多元:如果标签集有两个元素,叫做二元;如果含有更多元素,叫做多元\n",
    "* 分类器:有监督学习的分类问题中,面对带标签的数据,我们学习到的用来预测标签的模型叫做分类器\n",
    "* 生成性分类器:给出$P(X|Y)$的分类器,这样称呼是因为我们可以知道生成样本$X$的概率.比如贝叶斯分类器.\n",
    "* 判别性分类器:只给出$P(Y|X)$的分类器,因为我们只可以根据X来得到Y的概率来判别分类情况.比如逻辑回归和SVM.\n",
    "* 超平面:d维线性空间$S = \\{ \\vec{x} =(x_1,x_2,x_3,...,x_d), x_i \\in {-\\infty,\\infty} \\forall i \\in [1,d]\\}$中的$d-1$维子空间,比如$x_1*w_1 + x_2*w_2 + ... + x_d*w_d =-b$,通常$b=0$这个线性方程确定的平面就是d维空间中的一个超平面.注意,如果$b \\neq 0$,我们可以将d维数据增加一维,使得$x_(d+1) = 1, w_(d+1) =b$.\n",
    "* 线性可分:d维线性空间中,一组二元的d维数据集,如果可以根据标签被一个超平面完美得分割,我们就说这个数据集线性可分\n",
    "* 分隔超平面:一组带标签的d维数据线性可分,那个超平面就叫做分隔超平面\n",
    "* 线性分类器:分类问题中通过预测变量的线性组合来做出分类决策的模型\n",
    "* 学习理论:人工智能的一个分支,用来研究机器学习算法的设计和分析\n",
    "* 错误边界:在学习理论中,错误边界是一个机器学习算法收敛需要的更新数或者收敛前犯错数的上界\n",
    "* (点到超平面的)函数距离和几何距离:在一个d维空间S中,一个超平面H由$x_1*w_1 + x_2*w_2 + ... + x_d*w_d =0$决定,点$\\vec{x} =(a_1,a_2,a_3,...,a_d)$到H的函数距离为$||a_1*w_1 + a_2*w_2 + ... + a_d*w_d||$,几何距离为$\\frac {||a_1*w_1 + a_2*w_2 + ... + a_d*w_d||} {||(w_1,x_2,...,x_d)||}$\n",
    "* 凸子集:拓扑和几何上,如果一个集合S满足,$\\forall x_1, x_2 \\in S$,对于$\\forall 0<t<1$,$t*x_1+(1-t)*x_2 \\in S$,通俗地讲,如果集合内任何两点连成的直线都在集合内,则集合S为凸.\n",
    "* 凸函数:定义在某个线性空间的凸子集S上的函数f,如果满足 $\\forall x_1, x_2 \\in S, \\forall 0<t<1, f(t*x_1+(1-t)*x_2)<=  t*f(x_1)+(1-t)*f(x_2f)$,通俗地讲,如果函数图像任意两点之间的直线都在函数图像之上,则f为凸函数.\n",
    "* 优化:优化是数学的一个分支,求解一个函数的极值,有时会有等式或者不等式的约束条件.\n",
    "$min f(x)$\n",
    "\n",
    "subject to $ g_i(x) \\geq 0, i \\in [1,m]$\n",
    "\n",
    "$ h_j(x) = 0, j \\in [1,l]$\n",
    "\n",
    "其中$f(x)$叫做目标函数,$g_i(x), h_j(x)$分别为不等式和等式约束.当然,我们可以把等式约束用两个不等式约束$ h_j(x) \\geq 0,-h_j(x) \\geq 0 $代替.\n",
    "* 凸优化:优化中,当目标函数和不等式约束函数都是凸的,该优化就是凸优化.如果我们将等式约束单独列出,那么要求等式约束是仿射的.因为$ h_j(x) \\geq 0,-h_j(x) \\geq 0 $,可以得出 $h_j(x)$既凸又凹,所以是线性的.\n",
    "* 拉格朗日乘子法:只带等式约束的优化中,基于隐函数定理,通过给等式约束添加拉格朗日乘子,对各个变量和拉格朗日乘子求一阶导数,来求得极值的方法.\n",
    "* KKT条件:目标函数非线性有不等式约束条件下的优化中,存在最优解的一阶充要条件:目标函数和不等式约束函数为凸,等式约束仿射.\n",
    "* 前馈神经网络和循环神经网络:如图1,神经网络中神经元如果没有形成环,这个神经网络就属于前馈神经网络,反之则属于循环神经网络\n",
    "![图1](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/source/img/FeedForwardNeuralNetwork.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
