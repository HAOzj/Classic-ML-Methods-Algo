{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 简介和历史\n",
    "\n",
    "SVM中文叫做支持向量机,是一种监督学习方法,旨在找到具有最大几何间隔的**分隔超平面**,也就是找到一个线性或者非线性的分类器.原始版本的SVM线性分类器,由Vapnik在1963年提出,1992他又通过**核方法**引入了非线性分类器的可能.**支持向量**指的是离分隔超平面最近的点,我们下面会深入.\n",
    "\n",
    "## 动机\n",
    "\n",
    "根据[Novikoff定理](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf) 给一组带标签的数据$S = {x_i  \\in R^d, y_i \\in {0,1}, i=1,2,...,n}, \\forall x_i \\in S, R>=||x_i||$.假设$ \\exists \\gamma >0, w \\in R^d, s.t. \\gamma = \\min\\limits_{0<i<=n} \\frac{y_i (x_i*w)}{||w||}$.那么感知机算法的**错误边界**是$ \\frac{R^2}{\\gamma ^2}$. \n",
    "\n",
    "通俗地讲,一组带标签(标签可以是{0,1},也可以是{正,负})的数据有d个数字化的预测变量.几何上,我们把它视为d维点序列$S$,如果它线性可分,也就是存在一个d维向量w,$x*w =b$这个d-1维子空间,叫做**分隔超平面**,比所在线性空间低一维的子空间数学上叫做**超平面**,完美得根据标签分开了S.比如一组2维向量带有\"正\"\"负\"的标签,分布在在平面上,一条线完美得把所有的\"正\"点分在下面,\"负\"点分在上面,这条线就叫做分割超平面.\n",
    "\n",
    "$R$是数据点的最大长度,$\\gamma$是数据点到分割超平面的最小几何距离,感知机算法最多在$ \\frac{R^2}{\\gamma ^2}$个数据上犯错或者经过这么多次迭代,这个距离叫做**几何边界**,而犯错的数据数或者迭代数的最大值叫做**错误边界**.在例子里面,$R$就是数据点到原点的最大距离,$\\gamma$就是数据点到那条线的最小距离.\n",
    "\n",
    "所以,当我们面对一组(假设)线性可分的点序列时,最大化**几何边界**就可以最小化错误边界,就可以最小化[经验风险](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/lossFunction.ipynb).SVM方法最初就是对$\\gamma$这个**几何边界**的凸优化方法.\n",
    "\n",
    "## 算法介绍\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
