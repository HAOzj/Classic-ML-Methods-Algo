{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 简介和历史\n",
    "\n",
    "SVM中文叫做支持向量机,是一种监督学习方法,旨在找到具有最大几何间隔的[**分隔超平面**](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb),也就是找到一个[线性或者非线性](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb)的[分类器](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb).原始版本的SVM线性分类器,由Vapnik在1963年提出,1992他又通过[**核方法**](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb)引入了[非线性分类器](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb)的可能.**支持向量**指的是离分隔超平面最近的点,我们下面会深入.\n",
    "\n",
    "## 动机\n",
    "\n",
    "根据[Novikoff定理](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf) 给一组带标签的数据$S = {x_i  \\in R^d, y_i \\in {0,1}, i=1,2,...,n}, \\forall x_i \\in S, R>=||x_i||$.假设$ \\exists \\gamma >0, w \\in R^d, s.t. \\gamma = \\min\\limits_{0<i<=n} \\frac{y_i (x_i*w)}{||w||}$.那么[感知机算法](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/Perceptron.ipynb)的**[错误边界](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb)**是$ \\frac{R^2}{\\gamma ^2}$. \n",
    "\n",
    "通俗地讲,一组[带标签的d维数据](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb)[线性可分](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb).比如一组2维向量带有\"正\"\"负\"的标签,分布在在平面上,一条线完美得把所有的\"正\"点分在下面,\"负\"点分在上面,这条线是分割超平面.\n",
    "\n",
    "$R$是数据点的最大长度,$\\gamma$是数据点到分割超平面的最小[几何距离](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/BasicNotions.ipynb),感知机算法最多在$ \\frac{R^2}{\\gamma ^2}$个数据上犯错或者经过这么多次迭代,这个距离叫做**几何边界**.在例子里面,$R$就是数据点到原点的最大距离,$\\gamma$就是数据点到那条线的最小距离.\n",
    "\n",
    "所以,当我们面对一组(假设)线性可分的点序列时,最大化**几何边界**就可以最小化错误边界,就可以最小化[经验风险](https://github.com/HAOzj/Classic-ML-Methods-Algo/blob/master/ipynbs/supervised/lossFunction.ipynb).SVM方法最初就是对$\\gamma$这个**几何边界**的凸优化方法.\n",
    "\n",
    "## 算法介绍\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
