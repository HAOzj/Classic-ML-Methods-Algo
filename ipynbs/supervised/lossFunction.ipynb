{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "监督式机器学习算法是利用训练集,训练处一个模型,来获得一种关于分类或者回归的知识,来推广到新的数据.这里存在一个悖论,[bias-variance dilemma](https://en.wikipedia.org/wiki/Bias–variance_tradeoff),也就是监督式学习中不可能同时降低两种错误:第一种错误bias来源于欠拟合,也就是模型偏离了训练集;第二种错误variance是因为过拟合,也就是模型过度考虑了训练集而失去了推广能力.\n",
    "\n",
    "通俗地讲,如果一个模型太复杂,虽然它准确得拟合了测试集,但它的推广能力就要打折扣.如果一个模型太简单,也许可以推广到新的数据(新数据是未知的),但对训练集解释力很差.\n",
    "\n",
    "因为新数据是未知的,所以训练模型最常犯的错误是过拟合,为了降低过拟合的风险,我们有三种对策:\n",
    " 1. 损失函数添加正则化项,模型复杂度越高,正则化项越大.\n",
    " 2. 训练集拆分.也就是将训练集分为训练集,测试集.新的训练集一部分用来训练模型,测试集用来当做新数据,检验模型推广能力.\n",
    " 3. 交叉训练,也就是多次随机得将训练集拆分.\n",
    " \n",
    "这两类错误在模型训练时体现在损失函数$L(x,y, \\theta)$上,损失函数往往由两部分构成,经验风险$R_emp$和正则化项(模型复杂度的惩罚项)$r(d)$.\n",
    "\n",
    "其中经验风险对应的是模型在训练集上的错误,比如\n",
    " 1. Logistic中的对数损失$R_{emp} = \\sum -logP(y_i|x_i)$.\n",
    " 2. SVM中的hinge loss function $R_{emp} = \\sum max(0,1- \\hat{y_i}*y_i)$\n",
    " 3. 线性回归中的平方损失$R_{emp} = \\sum (\\hat{y_i}-y_i )^2$\n",
    " \n",
    "而正则化项$r(d)$随着模型的复杂度提高而增大,比如多项式模型的正则化项就比线性模型的大."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
